#include "HTSMatrix.h"
/* Author : mesriy at Tue Jul 24 15:56:45 2012
 * Generated by createNew
 */

#include <alien/kernels/hts/data_structure/HTSInternal.h>
#include <alien/kernels/hts/HTSBackEnd.h>

#include <alien/core/impl/MultiMatrixImpl.h>
#include <arccore/message_passing_mpi/MpiMessagePassingMng.h>

#ifdef ALIEN_USE_HTSSOLVER
#include "HARTSSolver/MatrixVector/CSR/CSRMatrixImpT.h"
#endif
/*---------------------------------------------------------------------------*/
BEGIN_HTSINTERNAL_NAMESPACE

bool HTSInternal::m_is_initialized = false;
int HTSInternal::m_nb_threads = 1;
std::size_t HTSInternal::m_nb_hyper_threads = 1;
std::size_t HTSInternal::m_mpi_core_id_offset = 0;
std::unique_ptr<HTSInternal::MachineInfoType> HTSInternal::m_machine_info;
std::unique_ptr<HTSInternal::ThreadSystemTopologyType> HTSInternal::m_topology;

std::unique_ptr<HTSInternal::ThreadEnvType> HTSInternal::m_thread_env;
std::unique_ptr<HTSInternal::MPIEnvType> HTSInternal::m_mpi_env;

template <>
int
HTSInternal::getEnv(std::string const& key, int default_value)
{
  const char* arch_str = ::getenv(key.c_str());
  if (arch_str)
    return atoi(arch_str);
  return default_value;
}

void
HTSInternal::initialize(Arccore::MessagePassing::IMessagePassingMng* parallel_mng)
{
  if (m_is_initialized)
    return;
  m_nb_threads = getEnv<int>("HTS_NUM_THREADS", 1);
  if (m_thread_env == nullptr) {
    m_thread_env.reset(new ThreadEnvType(m_nb_threads));
  }

  if (m_machine_info == nullptr) {
    m_machine_info.reset(new MachineInfoType());
    m_machine_info->init();
  }

  bool is_parallel = parallel_mng ? parallel_mng->commSize() > 1 : false;
  if (is_parallel && m_nb_threads > 1)
    m_mpi_core_id_offset = m_thread_env->getCurrentCoreId();
  typedef RunTimeSystem::AffinityMng<RunTimeSystem::MachineInfo> AffinityMngType;
  AffinityMngType::eMode set_aff = AffinityMngType::Block;

  AffinityMngType th_id_mng(
      *m_machine_info, set_aff, m_mpi_core_id_offset, -1, m_nb_hyper_threads);
  if (m_nb_threads > 1)
    m_thread_env->setAffinity(th_id_mng);

  if (m_topology.get() == nullptr) {
    struct RunTimeSystem::NumaThreadComparator<ThreadEnvType> numa_comp(
        m_thread_env.get());
    struct RunTimeSystem::DefaultThreadComparator def_comp;
    struct RunTimeSystem::DefaultCyclicThreadComparator defcyc_comp(m_nb_threads);
    int comp_case = getEnv<int>("HTS_THREAD_COMPARE_OPT", 0);
    switch (comp_case) {
    case 1:
      m_topology.reset(new ThreadSystemTopologyType(defcyc_comp, m_nb_threads));
      break;
    case 2:
      m_topology.reset(new ThreadSystemTopologyType(numa_comp, m_nb_threads));
      break;
    case 0:
    default:
      m_topology.reset(new ThreadSystemTopologyType(def_comp, m_nb_threads));
      break;
    }
  }

  if (parallel_mng && (parallel_mng->commSize() > 1)) {
    auto* mpi_mng =
        dynamic_cast<Arccore::MessagePassing::Mpi::MpiMessagePassingMng*>(parallel_mng);
    MPI_Comm comm = *static_cast<const MPI_Comm*>(mpi_mng->getMPIComm());
    initMPIEnv(comm);
  }
  m_is_initialized = true;
}

void
HTSInternal::initMPIEnv(MPI_Comm comm)
{

  m_mpi_env.reset(new HartsSolver::MPIInfo());
  m_mpi_env->init(comm, false); // external MPI management
}

void
HTSInternal::finalize()
{
  m_machine_info.reset();
  m_topology.reset();
  m_thread_env.reset();
  m_mpi_env.reset();
  m_is_initialized = false;
}

template <typename ValueT, bool is_mpi>
bool
MatrixInternal<ValueT, is_mpi>::initMatrix(
    Arccore::MessagePassing::IMessagePassingMng* parallel_mng, int nrows, int const* kcol,
    int const* cols, int block_size)
{
  if (is_mpi && parallel_mng)
    m_is_parallel = parallel_mng->commSize() > 1;

  ///////////////////////////////////////////////
  //
  // GET CURRENT PROFILE
  //

  if (m_is_parallel) {
    auto mpi_env = HTSInternal::getMPIEnv();
    assert(mpi_env);

    MPIPartitionType* mpi_partition = new MPIPartitionType(mpi_env->getParallelMng());
    mpi_partition->init(nrows);
    MCProfileViewType profile(kcol, cols, nrows);
    mpi_partition->compute(profile, true);
    m_partition_info.reset(mpi_partition);
  }

  if (m_profile.get() == nullptr) {
    if (m_partition_info) {
      m_profile_permutation.reset(new MCProfilePermType(*m_partition_info));
    }
    if (m_profile_permutation) {
      m_profile.reset(new MCProfileType());
      m_profile->init(nrows, kcol, cols, *m_profile_permutation);
    } else
      m_profile.reset(new MCProfileType(nrows, kcol, cols));
  }
  m_matrix.reset(new MCMatrixType(m_profile.get()));
  m_block_size = block_size;

  return true;
}

template <typename ValueT, bool is_mpi>
bool
MatrixInternal<ValueT, is_mpi>::setMatrixValues(Arccore::Real const* values)
{
  switch (m_block_size) {
  case 1: {
    m_matrix->setValues(values, m_profile_permutation.get());
    break;
  }
  default:
    break;
  }
  return true;
}

template <typename ValueT, bool is_mpi>
bool
MatrixInternal<ValueT, is_mpi>::computeDDMatrix()
{
  if (m_dd_matrix.get() == nullptr)
    m_dd_matrix.reset(new MCCSRMatrixType());

  int comm_rank = 0;
  std::size_t nrows = 0;
  PartitionFactory partition_factory;
  if (is_mpi && m_is_parallel) {
    auto mpi_env = HTSInternal::getMPIEnv();
    std::vector<int> const& domain_offset = m_partition_info->getDomainOffset();
    nrows = domain_offset[comm_rank + 1] - domain_offset[comm_rank];

    m_matrix->sortGhostCols(domain_offset, comm_rank);
    ProfileType& profile = m_matrix->getProfile();
    InfoVectorType& dist_cols = profile.getCols();
    InfoVectorType& dist_kcol = profile.getKCol();

    if (m_dist_info.get() == nullptr) {
      m_dist_info.reset(new HartsSolver::DistStructInfo());
      if (m_is_parallel) {
        m_dist_info->compute(domain_offset, mpi_env->getParallelMng(), profile, nullptr);
      } else {
        m_dist_info->m_local_row_size.resize(nrows);
        InfoVectorType const& kcol = profile.getKCol();
        for (std::size_t i = 0; i < nrows; ++i)
          m_dist_info->m_local_row_size[i] = kcol[i + 1] - kcol[i];
      }
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //
    // PARTITION DU DOMAIN LOCAL
    int mpi_interface_offset = m_partition_info->getDomainInterfaceOffset(comm_rank);
    int const* colsv = m_is_parallel ? m_dist_info->m_cols.data() : dist_cols.data();
    bool use_mpi_external_part = false;
    if (m_partition.get() == nullptr) {

      bool smetis = HTSInternal::getEnv<int>("HTS_SMETIS", 0) == 1;
      int nb_part = HTSInternal::getEnv<int>("HTS_NB_PART", 1);
      int nb_subpart = HTSInternal::getEnv<int>("HTS_NB_SUBPART", 1);
      std::size_t filter_size = mpi_interface_offset;
      std::size_t total_local_nrows = nrows;
      std::size_t ghost_size = 0.;
      if (nb_subpart == 0) {
        filter_size = nrows;
        nb_subpart = nb_part;
        ghost_size = m_dist_info->m_ghost_nrow;
      }

      HartsSolver::ProfileView local_profile(dist_kcol.data(), colsv, filter_size);
      Filter interface_filter(filter_size);
      m_partition.reset(partition_factory.create(local_profile, nrows, ghost_size,
          interface_filter, smetis, nb_subpart, true, nullptr));

      m_rs_partition.reset(new PartitionerType(m_partition.get()));
    }

    int sendrecv_opt = HTSInternal::getEnv<int>("HTS_SENDRECV_OPT", 3);
    switch (sendrecv_opt) {
    case 1:
      m_dd_matrix->setSendRecvPolicy(
          HartsSolver::CommProperty::Synch, HartsSolver::CommProperty::ASynch);
      break;
    case 2:
      m_dd_matrix->setSendRecvPolicy(
          HartsSolver::CommProperty::ASynch, HartsSolver::CommProperty::Synch);
      break;
    case 3:
      m_dd_matrix->setSendRecvPolicy(
          HartsSolver::CommProperty::ASynch, HartsSolver::CommProperty::ASynch);
      break;
    default:
      m_dd_matrix->setSendRecvPolicy(
          HartsSolver::CommProperty::Synch, HartsSolver::CommProperty::Synch);
      break;
    }

    if (m_dd_profile.get() == nullptr) {
      m_dd_matrix->initDD(*m_matrix, m_dist_info.get(), mpi_env->getParallelMng(),
          m_partition_info.get(), m_partition.get(), false, use_mpi_external_part,
          nullptr);
      m_dd_profile.reset(&m_dd_matrix->getDDProfile());
    } else {
      m_dd_matrix->init(m_dd_profile.get());
      typename MCCSRMatrixType::LocalProfileType const& profile = m_matrix->getProfile();
      m_dd_matrix->initDD(profile, m_dist_info.get(), mpi_env->getParallelMng(),
          m_partition_info.get(), m_partition.get(), false, nullptr);
      m_dd_matrix->updateValues(*m_matrix);
    }
  } else {
    if (m_partition.get() == nullptr) {
      bool smetis = HTSInternal::getEnv<int>("HTS_SMETIS", 0) == 1;
      int nb_part = HTSInternal::getEnv<int>("HTS_NB_PART", 1);
      int nb_subpart = HTSInternal::getEnv<int>("HTS_NB_SUBPART", 1);

      m_partition.reset(partition_factory.create(
          m_matrix->getProfile(), smetis, nb_subpart, 0, 0, 0, 0, 0, 0, false));

      m_rs_partition.reset(new PartitionerType(m_partition.get()));
    }
    // nrows = HartsSolver::numRows(*m_matrix) ;
    if (m_dd_profile.get() == nullptr) {
      m_dd_matrix->initDD(*m_matrix, *m_partition, false);
      m_dd_profile.reset(&m_dd_matrix->getDDProfile());
    } else {
      m_dd_matrix->init(m_matrix->getProfile(), m_dd_profile.get());
      m_dd_matrix->updateValues(*m_matrix);
    }
  }
  return true;
}

END_HTSINTERNAL_NAMESPACE

namespace Alien {
/*---------------------------------------------------------------------------*/
template <typename ValueT>
HTSMatrix<ValueT>::HTSMatrix(const MultiMatrixImpl* multi_impl)
: IMatrixImpl(multi_impl, AlgebraTraits<BackEnd::tag::hts>::name())
{
  const auto& row_space = multi_impl->rowSpace();
  const auto& col_space = multi_impl->colSpace();

  if (row_space.size() != col_space.size())
    throw Arccore::FatalErrorException("HTS matrix must be square");
}

/*---------------------------------------------------------------------------*/

template <typename ValueT> HTSMatrix<ValueT>::~HTSMatrix()
{
}

/*---------------------------------------------------------------------------*/

template <typename ValueT>
bool
HTSMatrix<ValueT>::initMatrix(Arccore::MessagePassing::IMessagePassingMng* parallel_mng,
    int nrows, int const* kcol, int const* cols, int block_size)
{
  HTSInternal::HTSInternal::initialize(parallel_mng);

  m_internal.reset(new MatrixInternal());
  return m_internal->initMatrix(parallel_mng, nrows, kcol, cols, block_size);
}

template <typename ValueT>
bool
HTSMatrix<ValueT>::setMatrixValues(Arccore::Real const* values)
{
  return m_internal->setMatrixValues(values);
}

template <typename ValueT>
bool
HTSMatrix<ValueT>::computeDDMatrix()
{
  return m_internal->computeDDMatrix();
}

template <typename ValueT>
void
HTSMatrix<ValueT>::mult(ValueT const* x, ValueT* y) const
{
  std::size_t alloc_size = m_internal->m_dd_matrix->allocSize();
  std::size_t nrows = HartsSolver::numRows(*m_internal->m_matrix);

  typename MatrixInternal::VectorType x2(alloc_size);
  typename MatrixInternal::VectorType y2(alloc_size);
  m_internal->m_dd_matrix->permut(nrows, x, x2.data());
  m_internal->m_dd_matrix->mult(x2, y2);
  m_internal->m_dd_matrix->invPermut(nrows, y2.data(), y);
}

template class HTSMatrix<double>;

} // namespace Alien

/*---------------------------------------------------------------------------*/
/*---------------------------------------------------------------------------*/
